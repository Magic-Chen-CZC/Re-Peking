#!/usr/bin/env python3
"""
æ•°æ®é‡‡é›†è„šæœ¬

åŠŸèƒ½ï¼šè´Ÿè´£æ•°æ®çš„"é‡‡é›† + æ¸…æ´— + å¯¼å‡ºåˆ° Excel"
æ”¯æŒæ•°æ®æºï¼šXHSï¼ˆå°çº¢ä¹¦ï¼‰ã€PDFï¼ˆæ–‡æ¡£ï¼‰ã€Webï¼ˆç½‘é¡µï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
    1. äº¤äº’å¼æ¨¡å¼ï¼ˆæ¨èï¼‰ï¼š
       python fetch_data.py
       
    2. å‘½ä»¤è¡Œæ¨¡å¼ï¼š
       python fetch_data.py --source xhs --file data/raw/xhs_notes.json
       python fetch_data.py --source pdf --file test_data/legends.pdf --doc_type legend
       python fetch_data.py --source web --url https://example.com/article --doc_type legend

è¾“å‡ºï¼š
    - Excel æ–‡ä»¶ä¿å­˜åœ¨ data/review/pending_{timestamp}.xlsx
    - éœ€è¦äººå·¥å®¡æ ¸åï¼Œä½¿ç”¨ build_db.py å¯¼å…¥æ•°æ®åº“
"""

import argparse
import asyncio
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Tuple, Optional

from config import settings
from utils.logger import logger

# å¯¼å…¥æ–°çš„ä¸šåŠ¡é…ç½®
from modules.domain_config import DOMAIN_CONFIG, get_domain_config, list_domain_types

# å¯¼å…¥çˆ¬è™«å’Œå¤„ç†å™¨
from modules.crawlers.xhs_crawler import XHSCrawler
from modules.crawlers.web_crawler import crawl_url
from modules.processors.xhs_processor import XHSProcessor
from modules.processors.pdf_processor import PDFProcessor

# å¯¼å…¥å®¡æ ¸æ¨¡å—
from modules.reviewer import export_to_excel


async def fetch_xhs_data(file_path: str) -> list:
    """
    é‡‡é›†å¹¶å¤„ç†å°çº¢ä¹¦æ•°æ®
    
    Args:
        file_path: æœ¬åœ° JSON æ–‡ä»¶è·¯å¾„
    
    Returns:
        å¤„ç†åçš„ XHSNote å¯¹è±¡åˆ—è¡¨
    """
    logger.info("=" * 80)
    logger.info("å¼€å§‹å¤„ç†å°çº¢ä¹¦æ•°æ®")
    logger.info("=" * 80)
    
    # åŠ è½½æœ¬åœ°æ•°æ®
    crawler = XHSCrawler(default_local_path=file_path)
    raw_notes = crawler.load_local_json_data(file_path)
    
    if not raw_notes:
        logger.warning("æœªè·å–åˆ°ä»»ä½•å°çº¢ä¹¦æ•°æ®")
        return []
    
    logger.info(f"åŠ è½½äº† {len(raw_notes)} æ¡åŸå§‹ç¬”è®°")
    
    # æ‰¹é‡å¤„ç†
    processor = XHSProcessor()
    xhs_notes = await processor.process_batch(raw_notes)
    
    logger.info(f"å¤„ç†å®Œæˆï¼Œå…± {len(xhs_notes)} æ¡æ•°æ®")
    
    return xhs_notes


def fetch_pdf_data(file_path: str, doc_type: str) -> list:
    """
    é‡‡é›†å¹¶å¤„ç† PDF æ•°æ®
    
    Args:
        file_path: PDF æ–‡ä»¶è·¯å¾„
        doc_type: æ–‡æ¡£ç±»å‹ï¼ˆlegend æˆ– archï¼‰
    
    Returns:
        å¤„ç†åçš„ StoryClip æˆ– ArchitectureDoc å¯¹è±¡åˆ—è¡¨
    """
    logger.info("=" * 80)
    logger.info("å¼€å§‹å¤„ç† PDF æ–‡æ¡£")
    logger.info("=" * 80)
    
    # å¤„ç† PDF
    processor = PDFProcessor()
    results = processor.process_pdf(
        file_path,
        doc_type=doc_type,
        save_intermediate=True  # ä¿å­˜ä¸­é—´ç»“æœä¾›è°ƒè¯•
    )
    
    logger.info(f"å¤„ç†å®Œæˆï¼Œå…± {len(results)} æ¡æœ‰æ•ˆæ•°æ®")
    
    return results


def fetch_web_data(url: str, doc_type: str) -> list:
    """
    é‡‡é›†å¹¶å¤„ç†ç½‘é¡µæ•°æ®
    
    Args:
        url: ç½‘é¡µ URL åœ°å€
        doc_type: æ–‡æ¡£ç±»å‹ï¼ˆlegendã€arch ç­‰ï¼‰
    
    Returns:
        å¤„ç†åçš„ç»“æ„åŒ–å†…å®¹åˆ—è¡¨
    """
    logger.info("=" * 80)
    logger.info("å¼€å§‹å¤„ç†ç½‘é¡µæ•°æ®")
    logger.info("=" * 80)
    
    # çˆ¬å–ç½‘é¡µå†…å®¹
    full_text = crawl_url(url)
    
    if not full_text or not full_text.strip():
        logger.warning(f"æœªèƒ½ä»ç½‘é¡µæå–å†…å®¹: {url}")
        return []
    
    logger.info(f"æˆåŠŸæå–ç½‘é¡µå†…å®¹ï¼Œå­—ç¬¦æ•°: {len(full_text)}")
    
    # è·å–ä¸šåŠ¡é…ç½®
    domain_config = get_domain_config(doc_type)
    if not domain_config:
        logger.error(f"ä¸æ”¯æŒçš„æ–‡æ¡£ç±»å‹: {doc_type}")
        return []
    
    # ä½¿ç”¨ PDF å¤„ç†å™¨çš„ç»Ÿä¸€æ–‡æœ¬å¤„ç†é€»è¾‘
    processor = PDFProcessor()
    
    # æ–‡æœ¬åˆ‡åˆ†ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„ chunking å‚æ•°ï¼‰
    chunks = processor._split_text(full_text, domain_config)
    
    if not chunks:
        logger.warning(f"æ–‡æœ¬åˆ‡åˆ†å¤±è´¥æˆ–è¿‡æ»¤åæ— æœ‰æ•ˆ chunk")
        return []
    
    logger.info(f"æ–‡æœ¬å·²åˆ‡åˆ†ä¸º {len(chunks)} ä¸ª Chunk")
    
    # å¤„ç† chunksï¼ˆä½¿ç”¨ç»Ÿä¸€çš„å¤„ç†é€»è¾‘ï¼‰
    results = processor._process_chunks(
        chunks,
        domain_config,
        url,  # ç”¨ URL ä½œä¸ºæ¥æºæ ‡è¯†
        doc_type
    )
    
    logger.info(f"å¤„ç†å®Œæˆï¼Œå…± {len(results)} æ¡æœ‰æ•ˆæ•°æ®")
    
    return results


# ============================================================================
# äº¤äº’å¼èœå•æ¨¡å¼ - é‡æ„ç‰ˆ
# ============================================================================

def print_banner():
    """æ‰“å°æ¬¢è¿æ¨ªå¹…"""
    print("\n" + "=" * 60)
    print("ğŸ¤– BeijingGuideAI æ•°æ®é‡‡é›†å‘å¯¼")
    print("=" * 60)


def select_data_source() -> Tuple[str, Optional[str], Optional[str]]:
    """
    ç¬¬ä¸€æ­¥ï¼šé€‰æ‹©æ•°æ®æ¥æº
    
    Returns:
        (loader_type, file_path_or_url, file_extension):
        - loader_type: 'pdf', 'json', 'web'
        - file_path_or_url: æ–‡ä»¶è·¯å¾„æˆ– URL
        - file_extension: æ–‡ä»¶æ‰©å±•åï¼ˆä»…æœ¬åœ°æ–‡ä»¶ï¼‰
    """
    print("\nã€æ­¥éª¤ 1/2ã€‘è¯·é€‰æ‹©æ•°æ®æ¥æºï¼š")
    print("[1] æœ¬åœ°æ–‡ä»¶ (Local File)")
    print("[2] ç½‘ç»œé“¾æ¥ (Web URL)")
    
    while True:
        try:
            choice = input("\nè¯·è¾“å…¥åºå· (1-2): ").strip()
            
            if choice == "1":
                # æœ¬åœ°æ–‡ä»¶åˆ†æ”¯
                return _select_local_file()
            elif choice == "2":
                # ç½‘ç»œé“¾æ¥åˆ†æ”¯
                return _select_web_url()
            else:
                print("âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ 1 æˆ– 2")
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å·²å–æ¶ˆ")
            sys.exit(0)


def _select_local_file() -> Tuple[str, str, str]:
    """
    é€‰æ‹©æœ¬åœ°æ–‡ä»¶
    
    Returns:
        (loader_type, file_path, file_extension)
    """
    data_dir = "data/raw"
    data_path = Path(data_dir)
    
    if not data_path.exists():
        os.makedirs(data_dir, exist_ok=True)
    
    # æ‰«ææ‰€æœ‰ééšè—æ–‡ä»¶
    all_files = [
        f for f in data_path.iterdir()
        if f.is_file() and not f.name.startswith('.')
    ]
    
    if not all_files:
        print(f"\nâŒ {data_dir}/ ç›®å½•ä¸‹æ²¡æœ‰æ–‡ä»¶")
        print(f"ğŸ’¡ è¯·å…ˆå°†æ•°æ®æ–‡ä»¶ï¼ˆ.pdf æˆ– .jsonï¼‰æ”¾å…¥è¯¥ç›®å½•")
        sys.exit(0)
    
    # æŒ‰æ–‡ä»¶åæ’åº
    all_files.sort(key=lambda x: x.name)
    
    print("\n" + "-" * 60)
    print("ğŸ“‚ å‘ç°ä»¥ä¸‹æ–‡ä»¶ï¼š")
    
    for i, file_path in enumerate(all_files, start=1):
        file_size = file_path.stat().st_size
        size_str = f"{file_size / 1024:.1f} KB" if file_size < 1024 * 1024 else f"{file_size / (1024 * 1024):.1f} MB"
        file_ext = file_path.suffix.lower()
        print(f"[{i}] {file_path.name} ({size_str}) {file_ext}")
    
    while True:
        try:
            choice = input(f"\nè¯·é€‰æ‹©è¦å¤„ç†çš„æ–‡ä»¶åºå· (1-{len(all_files)}): ").strip()
            
            if not choice:
                continue
            
            idx = int(choice) - 1
            
            if 0 <= idx < len(all_files):
                selected_file = all_files[idx]
                file_ext = selected_file.suffix.lower()
                
                # åˆ¤æ–­ loader_type
                if file_ext == '.pdf':
                    loader_type = 'pdf'
                elif file_ext == '.json':
                    loader_type = 'json'
                else:
                    print(f"âš ï¸  ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
                    print("   æ”¯æŒçš„æ ¼å¼: .pdf, .json")
                    continue
                
                return loader_type, str(selected_file), file_ext
            else:
                print(f"âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ 1-{len(all_files)} ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å·²å–æ¶ˆ")
            sys.exit(0)


def _select_web_url() -> Tuple[str, str, None]:
    """
    è¾“å…¥ç½‘ç»œ URL
    
    Returns:
        ('web', url, None)
    """
    print("\n" + "-" * 60)
    
    while True:
        try:
            url = input("ğŸ“ è¯·è¾“å…¥ç½‘é¡µ URL åœ°å€: ").strip()
            
            if not url:
                continue
            
            if not url.startswith(('http://', 'https://')):
                print("âŒ URL æ ¼å¼æ— æ•ˆï¼Œå¿…é¡»ä»¥ http:// æˆ– https:// å¼€å¤´")
                continue
            
            return 'web', url, None
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å·²å–æ¶ˆ")
            sys.exit(0)


def select_processing_strategy(loader_type: str) -> str:
    """
    ç¬¬äºŒæ­¥ï¼šé€‰æ‹©å¤„ç†ç­–ç•¥
    
    Args:
        loader_type: æ•°æ®åŠ è½½å™¨ç±»å‹ ('pdf', 'json', 'web')
    
    Returns:
        doc_type: ç­–ç•¥ç±»å‹ï¼ˆå¦‚ 'legend', 'arch', 'xhs'ï¼‰
    """
    print("\nã€æ­¥éª¤ 2/2ã€‘è¯·é€‰æ‹©è¯¥æ•°æ®çš„å¤„ç†ç­–ç•¥ï¼ˆå†…å®¹ç±»å‹ï¼‰ï¼š")
    
    # æ ¹æ® loader_type è¿‡æ»¤å¯ç”¨ç­–ç•¥
    if loader_type == 'json':
        # JSON æ–‡ä»¶åªèƒ½ç”¨ XHS ç­–ç•¥
        available_strategies = {'xhs': DOMAIN_CONFIG['xhs']}
    else:
        # PDF å’Œ Web å¯ä»¥ç”¨æ‰€æœ‰ç­–ç•¥ï¼ˆé™¤äº† xhsï¼‰
        available_strategies = {
            key: value for key, value in DOMAIN_CONFIG.items()
            if key != 'xhs'
        }
    
    # æ˜¾ç¤ºç­–ç•¥é€‰é¡¹
    strategy_list = list(available_strategies.items())
    for i, (key, config) in enumerate(strategy_list, start=1):
        print(f"[{i}] {key} - {config['description']}")
    
    while True:
        try:
            choice = input(f"\nè¯·è¾“å…¥åºå· (1-{len(strategy_list)}): ").strip()
            
            if not choice:
                continue
            
            idx = int(choice) - 1
            
            if 0 <= idx < len(strategy_list):
                doc_type = strategy_list[idx][0]
                return doc_type
            else:
                print(f"âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ 1-{len(strategy_list)} ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å·²å–æ¶ˆ")
            sys.exit(0)


async def run_interactive_mode():
    """è¿è¡Œäº¤äº’å¼èœå•æ¨¡å¼ - é‡æ„ç‰ˆ"""
    print_banner()
    
    # æ­¥éª¤ 1: é€‰æ‹©æ•°æ®æ¥æº
    loader_type, source_input, file_ext = select_data_source()
    
    # æ­¥éª¤ 2: é€‰æ‹©å¤„ç†ç­–ç•¥
    doc_type = select_processing_strategy(loader_type)
    
    # æ­¥éª¤ 3: æ‰§è¡Œå¤„ç†
    print("\n" + "-" * 60)
    source_type_name = {
        'pdf': 'PDF æ–‡æ¡£',
        'json': 'JSON æ–‡ä»¶',
        'web': 'ç½‘é¡µ'
    }.get(loader_type, 'æœªçŸ¥')
    
    strategy_name = DOMAIN_CONFIG[doc_type]['description']
    
    if loader_type == 'web':
        print(f"ğŸš€ å¼€å§‹å¤„ç†ï¼š{source_input}")
    else:
        print(f"ğŸš€ å¼€å§‹å¤„ç†ï¼š{Path(source_input).name}")
    
    print(f"   æ•°æ®ç±»å‹: {source_type_name}")
    print(f"   å¤„ç†ç­–ç•¥: {strategy_name}")
    print("-" * 60 + "\n")
    
    # æ ¹æ® (loader_type + doc_type) è°ƒç”¨å¯¹åº”çš„å¤„ç†é€»è¾‘
    results = []
    
    if loader_type == 'web':
        # Web + ä»»æ„ç­–ç•¥
        results = fetch_web_data(source_input, doc_type)
    
    elif loader_type == 'pdf':
        # PDF File + ä»»æ„ç­–ç•¥
        results = fetch_pdf_data(source_input, doc_type)
    
    elif loader_type == 'json' and doc_type == 'xhs':
        # JSON File + XHS ç­–ç•¥
        results = await fetch_xhs_data(source_input)
    
    else:
        # ä¸æ”¯æŒçš„ç»„åˆ
        logger.error(f"ä¸æ”¯æŒçš„ç»„åˆ: loader_type={loader_type}, doc_type={doc_type}")
        print(f"\nâŒ ä¸æ”¯æŒçš„æ•°æ®æºå’Œç­–ç•¥ç»„åˆ")
        return
    
    if not results:
        logger.warning("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
        print("\nâš ï¸  æœªæå–åˆ°æœ‰æ•ˆæ•°æ®")
        return
    
    # å¯¼å‡ºåˆ° Excel
    logger.info("=" * 80)
    logger.info("å¯¼å‡ºæ•°æ®åˆ° Excel")
    logger.info("=" * 80)
    
    excel_path = export_to_excel(results, output_dir="data/review")
    
    if excel_path:
        print("\n" + "=" * 80)
        print("âœ… æ•°æ®é‡‡é›†å’Œå¤„ç†å®Œæˆ")
        print("=" * 80)
        print(f"ğŸ“Š å…±å¤„ç† {len(results)} æ¡æ•°æ®")
        print(f"ğŸ“ Excel æ–‡ä»¶: {excel_path}")
        print("\nâš ï¸  è¯·åœ¨ Excel ä¸­äººå·¥å®¡æ ¸æ•°æ®ï¼š")
        print("   - æ£€æŸ¥ valid å­—æ®µï¼ˆTrue/Falseï¼‰")
        print("   - ä¿®æ”¹æˆ–åˆ é™¤ä¸åˆæ ¼çš„æ•°æ®")
        print("   - å®Œæˆåä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯¼å…¥æ•°æ®åº“ï¼š")
        print(f"\n   python build_db.py --file {excel_path}")
        print("=" * 80 + "\n")


async def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) == 1:
        # æ²¡æœ‰å‚æ•°ï¼Œå¯åŠ¨äº¤äº’å¼æ¨¡å¼
        await run_interactive_mode()
        return
    
    # æœ‰å‚æ•°ï¼Œä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å¼
    parser = argparse.ArgumentParser(
        description="åŒ—äº¬å¯¼è§ˆ AI - æ•°æ®é‡‡é›†è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  # å¤„ç†æœ¬åœ°å°çº¢ä¹¦æ•°æ®
  python fetch_data.py --source xhs --file data/raw/xhs_notes.json
  
  # å¤„ç† PDF ä¼ è¯´æ•…äº‹
  python fetch_data.py --source pdf --file test_data/legends.pdf --doc_type legend
  
  # å¤„ç† PDF å»ºç­‘æ–‡æ¡£
  python fetch_data.py --source pdf --file data/raw/architecture.pdf --doc_type arch
        """
    )
    
    parser.add_argument(
        "--source",
        type=str,
        required=True,
        choices=["xhs", "pdf", "web"],
        help="æ•°æ®æºç±»å‹ï¼šxhsï¼ˆå°çº¢ä¹¦ï¼‰ã€pdfï¼ˆPDFæ–‡æ¡£ï¼‰æˆ– webï¼ˆç½‘é¡µï¼‰"
    )
    
    parser.add_argument(
        "--file",
        type=str,
        help="æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆXHS çš„ JSON æ–‡ä»¶æˆ– PDF æ–‡ä»¶ï¼‰"
    )
    
    parser.add_argument(
        "--url",
        type=str,
        help="ç½‘é¡µ URL åœ°å€ï¼ˆä»…å½“ source=web æ—¶éœ€è¦ï¼‰"
    )
    
    parser.add_argument(
        "--doc_type",
        type=str,
        choices=["legend", "arch", "generic"],
        help="æ–‡æ¡£ç±»å‹ï¼šlegendï¼ˆä¼ è¯´æ•…äº‹ï¼‰ã€archï¼ˆå»ºç­‘æ–‡æ¡£ï¼‰æˆ– genericï¼ˆé€šç”¨ï¼‰"
    )
    
    parser.add_argument(
        "--output",
        type=str,
        default="data/review",
        help="Excel è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º data/review"
    )
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if args.source in ["xhs", "pdf"] and not args.file:
        logger.error("å¤„ç† XHS æˆ– PDF æ—¶å¿…é¡»æŒ‡å®š --file å‚æ•°")
        return
    
    if args.source == "web" and not args.url:
        logger.error("å¤„ç†ç½‘é¡µæ—¶å¿…é¡»æŒ‡å®š --url å‚æ•°")
        return
    
    if args.source in ["pdf", "web"] and not args.doc_type:
        logger.error("å¤„ç† PDF æˆ–ç½‘é¡µæ—¶å¿…é¡»æŒ‡å®š --doc_type (legend, arch æˆ– generic)")
        return
    
    if not Path(args.file).exists() if args.file else False:
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
        return
    
    # æ‰§è¡Œé‡‡é›†å’Œå¤„ç†
    results = []
    
    if args.source == "xhs":
        results = await fetch_xhs_data(args.file)
    elif args.source == "pdf":
        results = fetch_pdf_data(args.file, args.doc_type)
    elif args.source == "web":
        results = fetch_web_data(args.url, args.doc_type)
    
    if not results:
        logger.warning("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
        return
    
    # å¯¼å‡ºåˆ° Excel ä¾›äººå·¥å®¡æ ¸
    logger.info("=" * 80)
    logger.info("å¯¼å‡ºæ•°æ®åˆ° Excel")
    logger.info("=" * 80)
    
    excel_path = export_to_excel(results, output_dir=args.output)
    
    if excel_path:
        print("\n" + "=" * 80)
        print("âœ… æ•°æ®é‡‡é›†å’Œå¤„ç†å®Œæˆ")
        print("=" * 80)
        print(f"ğŸ“Š å…±å¤„ç† {len(results)} æ¡æ•°æ®")
        print(f"ğŸ“ Excel æ–‡ä»¶: {excel_path}")
        print("\nâš ï¸  è¯·åœ¨ Excel ä¸­äººå·¥å®¡æ ¸æ•°æ®ï¼š")
        print("   - æ£€æŸ¥ valid å­—æ®µï¼ˆTrue/Falseï¼‰")
        print("   - ä¿®æ”¹æˆ–åˆ é™¤ä¸åˆæ ¼çš„æ•°æ®")
        print("   - å®Œæˆåä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯¼å…¥æ•°æ®åº“ï¼š")
        print(f"\n   python build_db.py --file {excel_path}")
        print("=" * 80 + "\n")


if __name__ == "__main__":
    # åˆ¤æ–­æ˜¯å¦ä¸ºäº¤äº’å¼æ¨¡å¼
    if len(sys.argv) == 1:
        # æ— å‚æ•°ï¼Œè¿›å…¥äº¤äº’å¼æ¨¡å¼
        asyncio.run(run_interactive_mode())
    else:
        # æœ‰å‚æ•°ï¼Œè¿›å…¥å‘½ä»¤è¡Œæ¨¡å¼
        asyncio.run(main())
