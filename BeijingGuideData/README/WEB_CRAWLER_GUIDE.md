# ç½‘é¡µçˆ¬è™«åŠŸèƒ½ + æ–°äº¤äº’æ¨¡å¼è¯´æ˜

## ğŸ“‹ æ›´æ–°å†…å®¹

### 1. æ–°å¢ç½‘é¡µçˆ¬è™«æ”¯æŒ

ç°åœ¨å¯ä»¥ç›´æ¥ä»ç½‘é¡µ URL æŠ“å–å†…å®¹å¹¶è¿›è¡Œå¤„ç†ï¼

### 2. é‡æ„äº¤äº’å¼èœå•

å…¨æ–°çš„"ä¸¤æ­¥èµ°"äº¤äº’æµç¨‹ï¼š
1. **é€‰æ‹©æ•°æ®æ¥æº**ï¼ˆæœ¬åœ°æ–‡ä»¶ / ç½‘ç»œé“¾æ¥ï¼‰
2. **é€‰æ‹©å¤„ç†ç­–ç•¥**ï¼ˆè‡ªåŠ¨æ ¹æ®æ¥æºè¿‡æ»¤å¯ç”¨ç­–ç•¥ï¼‰

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ 1: äº¤äº’å¼æ¨¡å¼ï¼ˆæ¨èï¼‰

```bash
python fetch_data.py
```

**æ–°æµç¨‹**ï¼š

```
========================================
ğŸ¤– BeijingGuideAI æ•°æ®é‡‡é›†å‘å¯¼
========================================

ã€æ­¥éª¤ 1/2ã€‘è¯·é€‰æ‹©æ•°æ®æ¥æºï¼š
[1] æœ¬åœ°æ–‡ä»¶ (Local File)
[2] ç½‘ç»œé“¾æ¥ (Web URL)

è¯·è¾“å…¥åºå· (1-2): 2

------------------------------------------------------------
ğŸ“ è¯·è¾“å…¥ç½‘é¡µ URL åœ°å€: https://example.com/beijing-legend

ã€æ­¥éª¤ 2/2ã€‘è¯·é€‰æ‹©è¯¥æ•°æ®çš„å¤„ç†ç­–ç•¥ï¼ˆå†…å®¹ç±»å‹ï¼‰ï¼š
[1] legend - åŒ—äº¬å†å²æ•…äº‹å’Œä¼ è¯´çš„ç­›é€‰å’Œæå–
[2] arch - å»ºç­‘å’Œæ¶æ„æ–‡æ¡£çš„ä¸“ä¸šä¿¡æ¯æå–
[3] generic - é€šç”¨å†…å®¹æå–ï¼Œé€‚ç”¨äºæœªæ˜ç¡®åˆ†ç±»çš„æ•°æ®

è¯·è¾“å…¥åºå· (1-3): 1

------------------------------------------------------------
ğŸš€ å¼€å§‹å¤„ç†ï¼šhttps://example.com/beijing-legend
   æ•°æ®ç±»å‹: ç½‘é¡µ
   å¤„ç†ç­–ç•¥: åŒ—äº¬å†å²æ•…äº‹å’Œä¼ è¯´çš„ç­›é€‰å’Œæå–
------------------------------------------------------------
```

### æ–¹å¼ 2: å‘½ä»¤è¡Œæ¨¡å¼

```bash
# å¤„ç†ç½‘é¡µå†…å®¹
python fetch_data.py --source web --url https://example.com/article --doc_type legend

# å¤„ç†æœ¬åœ° PDF
python fetch_data.py --source pdf --file data/raw/legends.pdf --doc_type legend

# å¤„ç†å°çº¢ä¹¦ JSON
python fetch_data.py --source xhs --file data/raw/xhs_notes.json
```

---

## ğŸ“– è¯¦ç»†è¯´æ˜

### æ–°å¢æ¨¡å—

#### modules/crawlers/web_crawler.py

**åŠŸèƒ½**: ä»ç½‘é¡µ URL æå–æ–‡æœ¬å†…å®¹

**æ ¸å¿ƒå‡½æ•°**:
```python
def crawl_url(url: str, timeout: int = 10) -> str:
    """
    ä»ç½‘é¡µ URL æå–æ–‡æœ¬å†…å®¹
    
    æµç¨‹ï¼š
    1. ä½¿ç”¨ requests è·å– HTML
    2. ä½¿ç”¨ BeautifulSoup è§£æ
    3. æå–æ‰€æœ‰å¯è§æ–‡æœ¬
    4. æ¸…æ´—æ–‡æœ¬ï¼ˆå»é™¤å¤šä½™ç©ºè¡Œã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰
    """
```

**ç‰¹æ€§**:
- âœ… è‡ªåŠ¨å»é™¤ scriptã€styleã€meta ç­‰éå†…å®¹æ ‡ç­¾
- âœ… æ™ºèƒ½ç¼–ç æ£€æµ‹
- âœ… æ–‡æœ¬æ¸…æ´—ï¼ˆå»é™¤å¤šä½™ç©ºç™½ï¼‰
- âœ… å®Œå–„çš„å¼‚å¸¸å¤„ç†
- âœ… è‡ªå®šä¹‰ User-Agent

**æµ‹è¯•**:
```bash
python -m modules.crawlers.web_crawler https://example.com
```

### é‡æ„çš„äº¤äº’æµç¨‹

#### ç¬¬ä¸€æ­¥ï¼šé€‰æ‹©æ•°æ®æ¥æº

**é€‰é¡¹ 1 - æœ¬åœ°æ–‡ä»¶**:
- è‡ªåŠ¨æ‰«æ `data/raw/` ç›®å½•
- æ˜¾ç¤ºæ‰€æœ‰ééšè—æ–‡ä»¶
- è‡ªåŠ¨åˆ¤æ–­æ–‡ä»¶ç±»å‹ï¼ˆ.pdf â†’ pdf, .json â†’ jsonï¼‰
- æ˜¾ç¤ºæ–‡ä»¶å¤§å°

**é€‰é¡¹ 2 - ç½‘ç»œé“¾æ¥**:
- æç¤ºç”¨æˆ·è¾“å…¥å®Œæ•´ URL
- éªŒè¯ URL æ ¼å¼ï¼ˆå¿…é¡»ä»¥ http:// æˆ– https:// å¼€å¤´ï¼‰

#### ç¬¬äºŒæ­¥ï¼šé€‰æ‹©å¤„ç†ç­–ç•¥

**æ™ºèƒ½ç­–ç•¥è¿‡æ»¤**:
- JSON æ–‡ä»¶ â†’ åªæ˜¾ç¤º `xhs` ç­–ç•¥
- PDF/Web â†’ æ˜¾ç¤ºé™¤ `xhs` å¤–çš„æ‰€æœ‰ç­–ç•¥

**åŠ¨æ€ç­–ç•¥åˆ—è¡¨**:
ä» `modules/strategies.py` çš„ `PROCESSING_STRATEGIES` å­—å…¸è‡ªåŠ¨ç”Ÿæˆ

**å¯ç”¨ç­–ç•¥**:
1. `legend` - åŒ—äº¬å†å²æ•…äº‹å’Œä¼ è¯´çš„ç­›é€‰å’Œæå–
2. `arch` - å»ºç­‘å’Œæ¶æ„æ–‡æ¡£çš„ä¸“ä¸šä¿¡æ¯æå–
3. `generic` - é€šç”¨å†…å®¹æå–
4. `xhs` - å°çº¢ä¹¦ç¬”è®°å¤„ç†ï¼ˆä»… JSON æ–‡ä»¶ï¼‰

### å¤„ç†é€»è¾‘çŸ©é˜µ

| æ•°æ®æ¥æº | åŠ è½½å™¨ç±»å‹ | å¯ç”¨ç­–ç•¥ | å¤„ç†æµç¨‹ |
|---------|-----------|---------|---------|
| .json æ–‡ä»¶ | json | xhs | XHSCrawler â†’ XHSProcessor |
| .pdf æ–‡ä»¶ | pdf | legend, arch, generic | PDFLoader (OCR) â†’ PDFProcessor |
| ç½‘é¡µ URL | web | legend, arch, generic | WebCrawler â†’ PDFProcessor |

**æ³¨æ„**: ç½‘é¡µå’Œ PDF éƒ½ä½¿ç”¨ `PDFProcessor` çš„é€šç”¨æ–‡æœ¬å¤„ç†é€»è¾‘ã€‚

---

## ğŸ’¡ ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: çˆ¬å–ç½‘é¡µä¸Šçš„åŒ—äº¬ä¼ è¯´æ•…äº‹

```bash
python fetch_data.py
# é€‰æ‹© [2] ç½‘ç»œé“¾æ¥
# è¾“å…¥ URL: https://baike.baidu.com/item/åŒ—äº¬ä¼ è¯´
# é€‰æ‹© [1] legend
```

**è¾“å‡º**: `data/review/pending_*.xlsx`

### åœºæ™¯ 2: å¤„ç†æœ¬åœ° PDF å»ºç­‘æ–‡æ¡£

```bash
python fetch_data.py
# é€‰æ‹© [1] æœ¬åœ°æ–‡ä»¶
# é€‰æ‹© architecture.pdf
# é€‰æ‹© [2] arch
```

### åœºæ™¯ 3: æ‰¹é‡å¤„ç†å¤šä¸ªç½‘é¡µï¼ˆå‘½ä»¤è¡Œï¼‰

```bash
# è„šæœ¬ç¤ºä¾‹
for url in $(cat urls.txt); do
    python fetch_data.py --source web --url "$url" --doc_type legend
done
```

---

## ğŸ”§ æŠ€æœ¯å®ç°

### Web Crawler

**è¯·æ±‚å¤„ç†**:
```python
headers = {
    'User-Agent': 'Mozilla/5.0 ...'
}
response = requests.get(url, timeout=10, headers=headers)
```

**HTML è§£æ**:
```python
soup = BeautifulSoup(html_content, 'html.parser')

# ç§»é™¤éå†…å®¹æ ‡ç­¾
for script in soup(['script', 'style', 'meta', 'noscript']):
    script.decompose()

text = soup.get_text()
```

**æ–‡æœ¬æ¸…æ´—**:
```python
# æ›¿æ¢å¤šä¸ªç©ºç™½å­—ç¬¦ä¸ºå•ä¸ªç©ºæ ¼
text = re.sub(r'[ \t]+', ' ', text)

# æ›¿æ¢å¤šä¸ªæ¢è¡Œç¬¦ä¸ºåŒæ¢è¡Œç¬¦
text = re.sub(r'\n\s*\n+', '\n\n', text)

# è¿‡æ»¤ç©ºè¡Œ
lines = [line.strip() for line in text.split('\n') if line.strip()]
```

### Fetch Data é‡æ„

**æ–°å¢å‚æ•°**:
- `--url`: ç½‘é¡µ URL åœ°å€
- `--doc_type`: æ‰©å±•ä¸ºæ”¯æŒ `generic` ç­–ç•¥

**æ–°å¢å‡½æ•°**:
- `fetch_web_data()`: å¤„ç†ç½‘é¡µæ•°æ®
- `select_data_source()`: é€‰æ‹©æ•°æ®æ¥æº
- `select_processing_strategy()`: é€‰æ‹©å¤„ç†ç­–ç•¥
- `_select_local_file()`: æœ¬åœ°æ–‡ä»¶é€‰æ‹©é€»è¾‘
- `_select_web_url()`: URL è¾“å…¥é€»è¾‘

---

## ğŸ“Š å‘½ä»¤è¡Œå‚æ•°

### å®Œæ•´å‚æ•°åˆ—è¡¨

```bash
python fetch_data.py [--source SOURCE] [--file FILE] [--url URL] [--doc_type DOC_TYPE] [--output OUTPUT]
```

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `--source` | str | æ˜¯ | æ•°æ®æºç±»å‹ï¼šxhs, pdf, web |
| `--file` | str | æ¡ä»¶ | æ–‡ä»¶è·¯å¾„ï¼ˆsource=xhs/pdf æ—¶å¿…éœ€ï¼‰ |
| `--url` | str | æ¡ä»¶ | ç½‘é¡µ URLï¼ˆsource=web æ—¶å¿…éœ€ï¼‰ |
| `--doc_type` | str | æ¡ä»¶ | ç­–ç•¥ç±»å‹ï¼ˆsource=pdf/web æ—¶å¿…éœ€ï¼‰ |
| `--output` | str | å¦ | Excel è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ data/review |

### å‚æ•°éªŒè¯è§„åˆ™

1. `source=xhs` æˆ– `source=pdf` â†’ å¿…é¡»æä¾› `--file`
2. `source=web` â†’ å¿…é¡»æä¾› `--url`
3. `source=pdf` æˆ– `source=web` â†’ å¿…é¡»æä¾› `--doc_type`
4. `--file` æŒ‡å®šçš„æ–‡ä»¶å¿…é¡»å­˜åœ¨

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. ç½‘é¡µçˆ¬å–

**é€‰æ‹©åˆé€‚çš„ç½‘é¡µ**:
- âœ… æ–‡ç« ç±»é¡µé¢ï¼ˆå†…å®¹ä¸°å¯Œï¼‰
- âœ… ç™¾ç§‘ç±»é¡µé¢ï¼ˆç»“æ„åŒ–å†…å®¹ï¼‰
- âŒ åŠ¨æ€åŠ è½½é¡µé¢ï¼ˆJavaScript æ¸²æŸ“ï¼‰
- âŒ éœ€è¦ç™»å½•çš„é¡µé¢

**å¤„ç†ç­–ç•¥é€‰æ‹©**:
- å†å²æ•…äº‹ã€ä¼ è¯´ â†’ `legend`
- å»ºç­‘ä»‹ç»ã€æŠ€æœ¯æ–‡æ¡£ â†’ `arch`
- ä¸ç¡®å®šçš„å†…å®¹ â†’ `generic`

### 2. æœ¬åœ°æ–‡ä»¶

**æ–‡ä»¶ç»„ç»‡**:
```
data/raw/
â”œâ”€â”€ xhs_notes.json          # å°çº¢ä¹¦æ•°æ®
â”œâ”€â”€ legends.pdf             # ä¼ è¯´æ•…äº‹
â”œâ”€â”€ architecture.pdf        # å»ºç­‘æ–‡æ¡£
â””â”€â”€ web_archive.pdf         # ç½‘é¡µå­˜æ¡£
```

### 3. æ‰¹é‡å¤„ç†

**Shell è„šæœ¬ç¤ºä¾‹**:
```bash
#!/bin/bash
# batch_crawl.sh

# ä»æ–‡ä»¶è¯»å– URL åˆ—è¡¨
while IFS= read -r url; do
    echo "å¤„ç†: $url"
    python fetch_data.py --source web --url "$url" --doc_type legend
    sleep 2  # é¿å…è¯·æ±‚è¿‡å¿«
done < urls.txt
```

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: ç½‘é¡µçˆ¬å–å¤±è´¥

**ç—‡çŠ¶**: "è¯·æ±‚å¤±è´¥" æˆ– "ç½‘ç»œè¿æ¥å¤±è´¥"

**åŸå› **:
- URL æ ¼å¼é”™è¯¯
- ç½‘ç«™ä¸å¯è®¿é—®
- ç½‘ç»œè¿æ¥é—®é¢˜
- ç½‘ç«™æœ‰åçˆ¬è™«æœºåˆ¶

**è§£å†³**:
1. æ£€æŸ¥ URL æ ¼å¼ï¼ˆå¿…é¡»ä»¥ http:// æˆ– https:// å¼€å¤´ï¼‰
2. åœ¨æµè§ˆå™¨ä¸­æµ‹è¯• URL æ˜¯å¦å¯è®¿é—®
3. æ£€æŸ¥ç½‘ç»œè¿æ¥
4. å¯¹äºæœ‰åçˆ¬è™«çš„ç½‘ç«™ï¼Œè€ƒè™‘æ‰‹åŠ¨ä¿å­˜ä¸º PDF å†å¤„ç†

### é—®é¢˜ 2: æå–çš„æ–‡æœ¬å†…å®¹ä¸ºç©º

**ç—‡çŠ¶**: "æœªèƒ½ä»ç½‘é¡µæå–å†…å®¹"

**åŸå› **:
- ç½‘é¡µæ˜¯åŠ¨æ€åŠ è½½ï¼ˆJavaScript æ¸²æŸ“ï¼‰
- ç½‘é¡µå…¨æ˜¯å›¾ç‰‡ï¼Œæ²¡æœ‰æ–‡æœ¬
- ç½‘é¡µç»“æ„ç‰¹æ®Š

**è§£å†³**:
1. ä½¿ç”¨æµè§ˆå™¨"æŸ¥çœ‹æºä»£ç "ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ–‡æœ¬å†…å®¹
2. å¯¹äº JavaScript æ¸²æŸ“çš„ç½‘é¡µï¼Œå»ºè®®æ‰‹åŠ¨å¤åˆ¶ç²˜è´´åˆ°æ–‡æœ¬æ–‡ä»¶
3. è€ƒè™‘ä½¿ç”¨ Selenium ç­‰å·¥å…·ï¼ˆéœ€è¦é¢å¤–å¼€å‘ï¼‰

### é—®é¢˜ 3: ç­–ç•¥é€‰æ‹©ä¸å½“

**ç—‡çŠ¶**: Excel ä¸­å¤§é‡ `valid=False` çš„æ•°æ®

**åŸå› **:
- å†…å®¹ç±»å‹ä¸ç­–ç•¥ä¸åŒ¹é…
- ä¾‹å¦‚ï¼šå»ºç­‘æ–‡æ¡£ä½¿ç”¨äº† `legend` ç­–ç•¥

**è§£å†³**:
1. é‡æ–°è¿è¡Œï¼Œé€‰æ‹©æ­£ç¡®çš„ç­–ç•¥
2. ä½¿ç”¨ `generic` ç­–ç•¥ä½œä¸ºå›é€€é€‰é¡¹
3. åœ¨ Excel ä¸­æ‰‹åŠ¨æ ‡è®° `valid=True`

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [å®Œæ•´å·¥ä½œæµæŒ‡å—](WORKFLOW_GUIDE.md)
- [ç­–ç•¥å’Œæç¤ºè¯è¯´æ˜](README_PROMPTS_STRATEGIES.md)
- [PDF å¤„ç†è®¾ç½®](PDF_PROCESSING_SETUP.md)

---

## ğŸ“ æ›´æ–°æ—¥å¿—

**2025-12-11**:
- âœ… æ–°å¢ `modules/crawlers/web_crawler.py`
- âœ… é‡æ„ `fetch_data.py` äº¤äº’æµç¨‹
- âœ… æ·»åŠ  `--url` å‘½ä»¤è¡Œå‚æ•°
- âœ… æ”¯æŒåŠ¨æ€ç­–ç•¥é€‰æ‹©
- âœ… æ™ºèƒ½ç­–ç•¥è¿‡æ»¤ï¼ˆæ ¹æ®æ•°æ®æ¥æºï¼‰

---

**å¿«é€Ÿå¼€å§‹**: ç›´æ¥è¿è¡Œ `python fetch_data.py` ä½“éªŒæ–°åŠŸèƒ½ï¼
